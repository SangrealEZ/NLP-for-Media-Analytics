
##############################################################
# Load packages needed for analysis
##############################################################

# Parsing of HTML/XML files  
library(rvest)    

# String manipulation
library(stringr)   

# Eases DateTime manipulation
library(lubridate)

##############################################################
# Scrape web content
##############################################################

link <- read_html('https://www.smartindustry.com/resource-library/?nav=') # grab the link
title_html <- html_nodes(link,'h2') # find out node
title <- html_text(title_html) # parse it to text
head(title)

##############################################################
# Topic Modelling - LDA
##############################################################

library(tm)
library(topicmodels)
library(dplyr)
library(purrr)
library(ggplot2)
library(ggrepel)
library(tidytext)

# prepare / build a dtm 
a <- as.String(title)
docs <- Corpus(VectorSource(a))
mystop=c('my','didn','can')
dtm <- DocumentTermMatrix(docs, 
                          control=list(tolower=T, 
                                       removePunctuation=T, 
                                       removeNumbers=T, 
                                       stripWhitespace=T,
                                       stopwords=c(mystop,
                                                   stopwords("english"), 
                                                   stopwords("spanish"))))
dtm <- removeSparseTerms(dtm,0.996)
idx <- rowSums(as.matrix(dtm))>0
newdoc = docs[idx,]
dtm = dtm[idx,]
newdoc

# Build LDA models

## 1) Look for optimal number of topics
n_topics <- c(2:5) # check x topics
lda_compare <- n_topics %>%
  map(LDA,x = dtm, control = list(seed = 1109)) # build models with x topics(may take several minutes)
# plot perplexity
data_frame(k = n_topics,
           perplex = map_dbl(lda_compare, perplexity)) %>%
  ggplot(aes(k, perplex)) +
  geom_point() +
  geom_line() +
  labs(title = "Evaluating LDA topic models",
       subtitle = "Optimal number of topics (smaller is better)",
       x = "Number of topics",
       y = "Perplexity")
# The plot shows that model perplexity keeps decreasing when topic number reaches 20.
# However, model with 10 topics also have a satisfactory perplexity level.
# For interpretation, we will use a model with 10 topics.

## 2) Build LDA model
lda.model = LDA(dtm, 2)
perplexity(lda.model) # perplexity = 152.5049
myposterior <- posterior(lda.model) # get the posterior of the model

# topic distribution of each document, one row per document, one column per topic
topics = myposterior$topics # there are 8895 docs, and they contain 10 topics
# term distribution of each topic, one row per topic, one column per term
terms = myposterior$terms #189 terms distributed in 10 topics

## 3) Deep-dive analysis on the topics
# reshape the model into a one-topic-per-term-per-row format. 
lda_td <- tidy(lda.model)
head(lda_td,15)
# For each combination the model has Î², the probability of that term being generated from that topic.

# find top 20 terms for each topic
top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(20, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
head(top_terms,25)

# barplotting topic 1-5
top_terms[1:100,] %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta)) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE)  +
  coord_flip()
# barplotting topic 6-10
top_terms[101:200,] %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 2) +
  coord_flip()

# use ggplot to plot topic 1-10 in wordcloud
top_terms %>% 
  ggplot +
  aes(1,1, label = term) +
  geom_text_repel(segment.size = 0, force = 100) +
  scale_size(range = c(2, 15), guide = FALSE) +
  scale_y_discrete()+
  scale_x_discrete()+
  labs(x = '', y = '') +
  facet_wrap(~ topic, scales = "free", ncol = 2) +
  theme_classic()

# Check sample documents
tid = 10
myidx <- sample(which(topics[,tid]> max(topics[,tid])*0.8), 10) # get document index of those at 80th percentile of covering a given topic
newdoc[myidx]$"content"





